"""ì œí’ˆ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
import asyncio
import json
from src.product_evaluator import ProductEvaluator
from src.database import init_db

async def test_product_evaluation():
    """ì œí’ˆ í‰ê°€ ì‹œìŠ¤í…œ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì œí’ˆ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    init_db()
    
    # ProductEvaluator ì´ˆê¸°í™”
    try:
        evaluator = ProductEvaluator()
        print("âœ… ProductEvaluator ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ProductEvaluator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸í•  ì œí’ˆ ID
    test_product_id = 8  # ë§¥ìŠ¤ì»· í”„ë¡œ ì œí’ˆ
    
    print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ì œí’ˆ ID: {test_product_id}")
    print("-" * 50)
    
    # 1ë‹¨ê³„: ê°€ì¤‘í‰ê·  ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
    print("1ï¸âƒ£ ê°€ì¤‘í‰ê·  ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    weighted_score, calculation_details = evaluator.calculate_weighted_score(test_product_id)
    
    if weighted_score > 0:
        print(f"   âœ… ê°€ì¤‘í‰ê·  ì ìˆ˜: {weighted_score:.2f}/5.0")
        print(f"   ğŸ“Š ë¦¬ë·° ë¶„í¬:")
        
        for calc in calculation_details.get("calculations", []):
            rating = calc["rating"]
            count = calc["count"]
            weight = calc["weight"]
            contribution = calc["contribution"]
            print(f"      {rating}ì : {count}ê°œ Ã— {weight} = {contribution:.1f}")
        
        print(f"   ğŸ“ˆ ì´ ë¦¬ë·°: {calculation_details.get('total_reviews', 0)}ê°œ")
        print(f"   ğŸ§® ê°€ì¤‘ì¹˜ í•©: {calculation_details.get('weight_sum', 0):.1f}")
    else:
        print(f"   âŒ ê°€ì¤‘í‰ê·  ê³„ì‚° ì‹¤íŒ¨: {calculation_details}")
        return
    
    # 2ë‹¨ê³„: ëª¨ìˆœ íƒì§€ í…ŒìŠ¤íŠ¸
    print(f"\n2ï¸âƒ£ ìƒì„¸ì •ë³´-ë¦¬ë·° ëª¨ìˆœ íƒì§€ í…ŒìŠ¤íŠ¸")
    print("   â³ AI ëª¨ìˆœ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    
    try:
        contradictions, penalty_score = await evaluator.detect_contradictions(test_product_id)
        
        print(f"   âœ… ëª¨ìˆœ íƒì§€ ì™„ë£Œ!")
        print(f"   ğŸ” ë°œê²¬ëœ ëª¨ìˆœ: {len(contradictions)}ê°œ")
        print(f"   ğŸ“‰ ì ìˆ˜ ì°¨ê°: -{penalty_score:.2f}ì ")
        
        if contradictions:
            print(f"   ğŸ“‹ ëª¨ìˆœ ìƒì„¸:")
            for i, contradiction in enumerate(contradictions, 1):
                severity = contradiction.get("severity", "unknown")
                claim = contradiction.get("claim", "N/A")
                reality = contradiction.get("reality", "N/A")
                contradiction_type = contradiction.get("type", "ê¸°íƒ€")
                
                print(f"      {i}. [{severity.upper()}] {contradiction_type}")
                print(f"         ì£¼ì¥: {claim}")
                print(f"         í˜„ì‹¤: {reality}")
        else:
            print("   âœ… ëª¨ìˆœì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"   âŒ ëª¨ìˆœ íƒì§€ ì‹¤íŒ¨: {e}")
        penalty_score = 0.0
        contradictions = []
    
    # 3ë‹¨ê³„: ì¢…í•© í‰ê°€ ì‹¤í–‰
    print(f"\n3ï¸âƒ£ ì¢…í•© í‰ê°€ ì‹¤í–‰")
    print("   â³ ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘...")
    
    try:
        evaluation_result = await evaluator.evaluate_product(test_product_id)
        
        if "error" in evaluation_result:
            print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {evaluation_result['error']}")
            return
        
        print(f"   âœ… ì¢…í•© í‰ê°€ ì™„ë£Œ!")
        
        # ê²°ê³¼ ì¶œë ¥
        final_score = evaluation_result["final_score"]
        weighted_score = evaluation_result["weighted_score"]
        penalty_score = evaluation_result["penalty_score"]
        grade = evaluation_result["evaluation_summary"]["evaluation_grade"]
        
        print(f"\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
        print(f"   ğŸ¯ ê°€ì¤‘í‰ê·  ì ìˆ˜: {weighted_score:.1f}/100ì ")
        print(f"   âš ï¸ ëª¨ìˆœ ì°¨ê°: -{penalty_score:.1f}ì ")
        print(f"   ğŸ† ìµœì¢… ì ìˆ˜: {final_score:.1f}/100ì ")
        print(f"   ğŸ“ˆ í‰ê°€ ë“±ê¸‰: {grade}")
        
    except Exception as e:
        print(f"   âŒ ì¢…í•© í‰ê°€ ì‹¤íŒ¨: {e}")
        return
    
    # 4ë‹¨ê³„: ì €ì¥ëœ ê²°ê³¼ í™•ì¸
    print(f"\n4ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²°ê³¼ í™•ì¸")
    evaluation_summary = evaluator.get_evaluation_summary(test_product_id)
    
    if evaluation_summary:
        print(f"   âœ… í‰ê°€ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨")
        print(f"   ì œí’ˆëª…: {evaluation_summary['product_name']}")
        print(f"   ìµœì¢… ì ìˆ˜: {evaluation_summary['final_score']:.1f}/100ì ")
        print(f"   ë“±ê¸‰: {evaluation_summary['grade']}")
        print(f"   ëª¨ìˆœ ê°œìˆ˜: {evaluation_summary['contradictions_count']}ê°œ")
        print(f"   í‰ê°€ ì¼ì‹œ: {evaluation_summary['evaluated_at']}")
    else:
        print(f"   âŒ ì €ì¥ëœ í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 5ë‹¨ê³„: ì „ì²´ í†µê³„ í™•ì¸
    print(f"\n5ï¸âƒ£ ì „ì²´ í‰ê°€ í†µê³„")
    stats = await evaluator.get_evaluation_stats()
    
    if "error" not in stats and "message" not in stats:
        print(f"   ì´ í‰ê°€ ì œí’ˆ: {stats['total_evaluated']}ê°œ")
        print(f"   í‰ê·  ì ìˆ˜: {stats['average_score']:.1f}/100ì ")
        print(f"   ìµœê³  ì ìˆ˜: {stats['highest_score']:.1f}/100ì ")
        print(f"   ìµœì € ì ìˆ˜: {stats['lowest_score']:.1f}/100ì ")
        
        print(f"   ğŸ“Š ë“±ê¸‰ ë¶„í¬:")
        for grade, count in stats['grade_distribution'].items():
            print(f"      {grade}: {count}ê°œ")
        
        print(f"   ğŸ† ìƒìœ„ ì œí’ˆ:")
        for i, product in enumerate(stats['top_products'], 1):
            print(f"      {i}. {product['name']}: {product['final_score']:.1f}ì  ({product['grade']})")
    else:
        print(f"   ğŸ“Š í†µê³„: {stats.get('message', stats.get('error', 'Unknown'))}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ì œí’ˆ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

async def test_weighted_score_only():
    """ê°€ì¤‘í‰ê·  ì ìˆ˜ ê³„ì‚°ë§Œ í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)"""
    print("\nğŸ” ê°€ì¤‘í‰ê·  ì ìˆ˜ ê³„ì‚° ìƒì„¸ í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    evaluator = ProductEvaluator()
    
    # ê°€ì¤‘ì¹˜ ì„¤ì • í™•ì¸
    print("âš–ï¸ ê°€ì¤‘ì¹˜ ì„¤ì •:")
    for rating, weight in evaluator.rating_weights.items():
        print(f"   {rating}ì : {weight}")
    
    # í…ŒìŠ¤íŠ¸ ê³„ì‚°
    weighted_score, details = evaluator.calculate_weighted_score(8)
    
    print(f"\nğŸ“Š ê³„ì‚° ê²°ê³¼:")
    print(f"   ê°€ì¤‘í‰ê· : {weighted_score:.2f}/5.0")
    print(f"   ì´ ë¦¬ë·°: {details.get('total_reviews', 0)}ê°œ")
    
    # ì¼ë°˜ í‰ê· ê³¼ ë¹„êµ
    total_score = 0
    total_count = 0
    for calc in details.get("calculations", []):
        rating = calc["rating"]
        count = calc["count"]
        total_score += rating * count
        total_count += count
    
    if total_count > 0:
        simple_average = total_score / total_count
        print(f"   ì¼ë°˜ í‰ê· : {simple_average:.2f}/5.0")
        print(f"   ì°¨ì´: {weighted_score - simple_average:.2f}ì ")

if __name__ == "__main__":
    # ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    asyncio.run(test_product_evaluation())
    
    # ê°€ì¤‘í‰ê·  ìƒì„¸ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    # asyncio.run(test_weighted_score_only())